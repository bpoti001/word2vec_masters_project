{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import struct\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import itertools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VocabItem:\n",
    "    def __init__(self, word):\n",
    "        self.word = word\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, fi, min_count):\n",
    "        vocab_items = []\n",
    "        vocab_hash = {}\n",
    "        word_count = 0\n",
    "        fi = open(fi, 'r')\n",
    "        # Add special tokens <bol> (beginning of line) and <eol> (end of line)\n",
    "        for token in ['<bol>', '<eol>']:\n",
    "            vocab_hash[token] = len(vocab_items)\n",
    "            vocab_items.append(VocabItem(token))\n",
    "        for line in fi:\n",
    "            tokens = line.split()\n",
    "            #print(\"\\rReading line %s\" %tokens)\n",
    "            for token in tokens:\n",
    "                if token not in vocab_hash:\n",
    "                    vocab_hash[token] = len(vocab_items)\n",
    "                    #print (\"\\r\\r token %s\" %token)\n",
    "                    #print (\"\\t\\t token value\",vocab_hash[token])\n",
    "                    vocab_items.append(VocabItem(token))\n",
    "                #assert vocab_items[vocab_hash[token]].word == token, 'Wrong vocab_hash index'\n",
    "                vocab_items[vocab_hash[token]].count += 1\n",
    "                word_count += 1\n",
    "                if word_count % 10000 == 0:\n",
    "                    sys.stdout.write(\"\\rReading word %d\" % word_count)\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "            # Add special tokens <bol> (beginning of line) and <eol> (end of line)\n",
    "            vocab_items[vocab_hash['<bol>']].count += 1\n",
    "            vocab_items[vocab_hash['<eol>']].count += 1\n",
    "            word_count += 2\n",
    "        self.vocab_items = vocab_items # List of VocabItem objects\n",
    "        self.vocab_hash = vocab_hash  # Mapping from each token to its index in vocab\n",
    "        self.word_count = word_count # Total number of words in train file\n",
    "        # Add special token <unk> (unknown),\n",
    "        # merge words occurring less than min_count into <unk>, and\n",
    "        # sort vocab in descending order by frequency in train file\n",
    "        self.__sort(min_count)\n",
    "        #assert self.word_count == sum([t.count for t in self.vocab_items]), 'word_count and sum of t.count do not agree'\n",
    "        print ('Total words in training file: %d' % self.word_count)\n",
    "        #print ('Total bytes in training file: %d' % self.bytes)\n",
    "        print ('Vocab size: %d' % len(self))\n",
    "    def __getitem__(self, i):\n",
    "        return self.vocab_items[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab_items)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.vocab_items)\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return key in self.vocab_hash\n",
    "\n",
    "    def __sort(self, min_count):\n",
    "        tmp = []\n",
    "        tmp.append(VocabItem('<unk>'))\n",
    "        unk_hash = 0\n",
    "        \n",
    "        count_unk = 0\n",
    "        for token in self.vocab_items:\n",
    "            if token.count < min_count:\n",
    "                count_unk += 1\n",
    "                tmp[unk_hash].count += token.count\n",
    "                #print(\"word setting as unknow:\",token.word)\n",
    "            else:\n",
    "                tmp.append(token)\n",
    "\n",
    "        tmp.sort(key=lambda token : token.count, reverse=True)\n",
    "\n",
    "        # Update vocab_hash\n",
    "        vocab_hash = {}\n",
    "        for i, token in enumerate(tmp):\n",
    "            vocab_hash[token.word] = i\n",
    "\n",
    "        self.vocab_items = tmp\n",
    "        self.vocab_hash = vocab_hash\n",
    "        #print (\"printing vocab_hash\")\n",
    "        #for key,value in vocab_hash.items():\n",
    "         #   print (key,value)\n",
    "        #print ('Unknown vocab size:', count_unk)\n",
    "\n",
    "    def indices(self, tokens):\n",
    "        return [self.vocab_hash[token] if token in self else self.vocab_hash['<unk>'] for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UnigramTable:\n",
    "    \"\"\"\n",
    "    A list of indices of tokens in the vocab following a power law distribution,\n",
    "    used to draw negative samples.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab):\n",
    "        vocab_size = len(vocab)\n",
    "        power = 0.75\n",
    "        norm = sum([math.pow(t.count, power) for t in vocab]) # Normalizing constant\n",
    "        print (norm)\n",
    "        table_size = 1e8 # Length of the unigram table depends on vocab\n",
    "        #print table_size\n",
    "        table = np.zeros(table_size, dtype=np.uint32)\n",
    "\n",
    "        print ('Filling unigram table')\n",
    "        p = 0 # Cumulative probability\n",
    "        i = 0\n",
    "        old_i = 0 \n",
    "        for j, unigram in enumerate(vocab):\n",
    "            #print \"j\",j\n",
    "            #print \"unigram\",unigram\n",
    "            \n",
    "            p += float(math.pow(unigram.count, power))/norm\n",
    "            while i < table_size and float(i) / table_size < p:\n",
    "                table[i] = j\n",
    "                i += 1\n",
    "            old_i = i - old_i\n",
    "            sys.stdout.write(\"\\r propability for word '%s' is %f, kept it  %d times\" %(unigram.word,p,old_i))\n",
    "            sys.stdout.flush()\n",
    "            #print(\"propability for word %s is %f, kept it  %d times\" %(unigram.word,p,old_i))\n",
    "        self.table = table\n",
    "    def sample(self, count):\n",
    "        indices = np.random.randint(low=0, high=len(self.table), size=count)\n",
    "        return [self.table[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z): #sigmoid function goes from -6 to +6\n",
    "    if z > 6:\n",
    "        return 1.0\n",
    "    elif z < -6:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1 / (1 + math.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_net(dim, vocab_size):\n",
    "    # Init input words with random numbers from a uniform distribution on the interval [-0.5, 0.5]/dim\n",
    "    tmp = np.random.uniform(low=-0.5/dim, high=0.5/dim, size=(vocab_size, dim))\n",
    "    input_word = tmp \n",
    "    # Init weights with zeros\n",
    "    tmp = np.zeros(shape=(vocab_size, dim))\n",
    "    output_word = tmp\n",
    "  \n",
    "\n",
    "    return (input_word, output_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading word 17000000Total words in training file: 17005209\n",
      "Vocab size: 47135\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary(\"text8\",10) #create vocab with count of skip words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2159669.0869037905\n",
      "Filling unigram table\n",
      " propability for word 'kirchenmusik' is 1.000000, kept it  50420761 times"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:13: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "table = UnigramTable(vocab) #create unigram table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.013621 processed 7740000 words Progress: 7740000 of 17005209 (45.52%)"
     ]
    }
   ],
   "source": [
    "\n",
    "input_word,output_word = init_net(5, len(vocab)) #initalize input words and weight matrix with 5 diminension for each word \n",
    "\n",
    "current_word_count=0  #count of words to update alpha\n",
    "alpha_count = 0\n",
    "last_alpha_count = 0\n",
    "starting_alpha=0.025 #initial alpha\n",
    "win=5 #max window size\n",
    "#iter_c = 0 #iteration count\n",
    "with open(\"text8\", 'r') as fi:\n",
    "    for lines in fi: #for each line get the indicies of the words\n",
    "        line = lines.strip()\n",
    "        #print (line)\n",
    "        sent = vocab.indices(['<bol>'] + line.split() + ['<eol>'])\n",
    "        #print (\"sent\",sent)\n",
    "        for sent_pos, token in enumerate(sent): #use the position of the \"input word\" to create context window randomly\n",
    "            #print (\"for sent_pos,indices\",sent_pos,token)\n",
    "            #update alpha for every 1000 words and print status\n",
    "            if current_word_count % 1000 == 0:\n",
    "                alpha_count += (current_word_count - last_alpha_count)\n",
    "                last_alpha_count = current_word_count\n",
    "                alpha = starting_alpha * (1 - float(alpha_count) / vocab.word_count)\n",
    "                if alpha < starting_alpha * 0.0001: alpha = starting_alpha * 0.0001\n",
    "                sys.stdout.write(\"\\rAlpha: %f processed %d words Progress: %d of %d (%.2f%%)\" %\n",
    "                                 (alpha, current_word_count, alpha_count, vocab.word_count,\n",
    "                                  float(alpha_count) / vocab.word_count * 100))\n",
    "                sys.stdout.flush()\n",
    "            \n",
    "            #compute the random context\n",
    "            current_win = np.random.randint(low=1, high=win+1)\n",
    "            context_start = max(sent_pos - current_win, 0)\n",
    "            context_end = min(sent_pos + current_win + 1, len(sent))\n",
    "            #print (\"context window\")\n",
    "            #print (\"starting\",context_start,\"ending\",context_end)\n",
    "            \n",
    "            #get the words which are either side of the input word\n",
    "            context = sent[context_start:sent_pos] + sent[sent_pos+1:context_end]\n",
    "            #print (\"length of the context is\",len(context))\n",
    "            #print (context)\n",
    "            \n",
    "            #for every context word\n",
    "            for context_word in context:\n",
    "                error_back = np.zeros(5)\n",
    "                # classifiy inputword with 1 and sample words taken from unigram table as 0\n",
    "                classifiers = [(token, 1)] + [(target, 0) for target in table.sample(2)]\n",
    "                #print(\"classifiers\",classifiers)\n",
    "                \n",
    "                #for every word in classifier\n",
    "                for target, label in classifiers:\n",
    "                    #print (\"\\ttarget\",target)\n",
    "                    #print (\"\\t input word[context_word]\",input_word[context_word])\n",
    "                    #print (\"\\t output word [target]\",output_word[target])\n",
    "                    \n",
    "                    #get dot product of input word vector of context word and weight vector of classifier word\n",
    "                    z = np.dot(input_word[context_word], output_word[target])\n",
    "                    #print (\"\\tdot product\",z)\n",
    "                    \n",
    "                    #compute sigmoid of dot product\n",
    "                    p = sigmoid(z)\n",
    "                    #print (\"\\tsigmoid\",p)\n",
    "                    \n",
    "                    #gradient\n",
    "                    g = alpha * (label - p)\n",
    "                    #print (\"\\tgradient\",g)\n",
    "                    \n",
    "                    #sum up the error computed for every word in the classifier\n",
    "                    error_back += g * output_word[target]# Error to backpropagate to input word vector\n",
    "                    #print(\"\\terror\",neu1e)\n",
    "                    #update weight of the words in classifer\n",
    "                    output_word[target] -= g * input_word[context_word]# Update output word\n",
    "                    \n",
    "                    #print (\"\\tupdated array of target\",target,\"is\",output_word[target])\n",
    "                    #iter_c=iter_c+1\n",
    "                        # Update input_word\n",
    "                #update input word vector of the context using summed error of the classfier\n",
    "                input_word[context_word] -= error_back\n",
    "                #print(\"updated array of context word\",context_word,\"is \",input_word[context_word])\n",
    "            current_word_count += 1\n",
    "#print (\"iteration count\",iter_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
