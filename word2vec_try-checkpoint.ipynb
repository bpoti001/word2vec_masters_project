{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import struct\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import itertools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VocabItem:\n",
    "    def __init__(self, word):\n",
    "        self.word = word\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, fi, min_count):\n",
    "        vocab_items = []\n",
    "        vocab_hash = {}\n",
    "        word_count = 0\n",
    "        fi = open(fi, 'r')\n",
    "        # Add special tokens <bol> (beginning of line) and <eol> (end of line)\n",
    "        for token in ['<bol>', '<eol>']:\n",
    "            vocab_hash[token] = len(vocab_items)\n",
    "            vocab_items.append(VocabItem(token))\n",
    "        for line in fi:\n",
    "            tokens = line.split()\n",
    "            #print(\"\\rReading line %s\" %tokens)\n",
    "            for token in tokens:\n",
    "                if token not in vocab_hash:\n",
    "                    vocab_hash[token] = len(vocab_items)\n",
    "                    #print (\"\\r\\r token %s\" %token)\n",
    "                    #print (\"\\t\\t token value\",vocab_hash[token])\n",
    "                    vocab_items.append(VocabItem(token))\n",
    "                #assert vocab_items[vocab_hash[token]].word == token, 'Wrong vocab_hash index'\n",
    "                vocab_items[vocab_hash[token]].count += 1\n",
    "                word_count += 1\n",
    "                if word_count % 10000 == 0:\n",
    "                    sys.stdout.write(\"\\rReading word %d\" % word_count)\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "            # Add special tokens <bol> (beginning of line) and <eol> (end of line)\n",
    "            vocab_items[vocab_hash['<bol>']].count += 1\n",
    "            vocab_items[vocab_hash['<eol>']].count += 1\n",
    "            word_count += 2\n",
    "        self.vocab_items = vocab_items # List of VocabItem objects\n",
    "        self.vocab_hash = vocab_hash  # Mapping from each token to its index in vocab\n",
    "        self.word_count = word_count # Total number of words in train file\n",
    "        # Add special token <unk> (unknown),\n",
    "        # merge words occurring less than min_count into <unk>, and\n",
    "        # sort vocab in descending order by frequency in train file\n",
    "        self.__sort(min_count)\n",
    "        #assert self.word_count == sum([t.count for t in self.vocab_items]), 'word_count and sum of t.count do not agree'\n",
    "        print ('Total words in training file: %d' % self.word_count)\n",
    "        #print ('Total bytes in training file: %d' % self.bytes)\n",
    "        print ('Vocab size: %d' % len(self))\n",
    "    def __getitem__(self, i):\n",
    "        return self.vocab_items[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab_items)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.vocab_items)\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return key in self.vocab_hash\n",
    "\n",
    "    def __sort(self, min_count):\n",
    "        tmp = []\n",
    "        tmp.append(VocabItem('<unk>'))\n",
    "        unk_hash = 0\n",
    "        \n",
    "        count_unk = 0\n",
    "        for token in self.vocab_items:\n",
    "            if token.count < min_count:\n",
    "                count_unk += 1\n",
    "                tmp[unk_hash].count += token.count\n",
    "                #print(\"word setting as unknow:\",token.word)\n",
    "            else:\n",
    "                tmp.append(token)\n",
    "\n",
    "        tmp.sort(key=lambda token : token.count, reverse=True)\n",
    "\n",
    "        # Update vocab_hash\n",
    "        vocab_hash = {}\n",
    "        for i, token in enumerate(tmp):\n",
    "            vocab_hash[token.word] = i\n",
    "\n",
    "        self.vocab_items = tmp\n",
    "        self.vocab_hash = vocab_hash\n",
    "        #print (\"printing vocab_hash\")\n",
    "        #for key,value in vocab_hash.items():\n",
    "         #   print (key,value)\n",
    "        #print ('Unknown vocab size:', count_unk)\n",
    "\n",
    "    def indices(self, tokens):\n",
    "        return [self.vocab_hash[token] if token in self else self.vocab_hash['<unk>'] for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UnigramTable:\n",
    "    \"\"\"\n",
    "    A list of indices of tokens in the vocab following a power law distribution,\n",
    "    used to draw negative samples.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab):\n",
    "        vocab_size = len(vocab)\n",
    "        power = 0.75\n",
    "        norm = sum([math.pow(t.count, power) for t in vocab]) # Normalizing constant\n",
    "        print (norm)\n",
    "        table_size = 1e8 # Length of the unigram table depends on vocab\n",
    "        #print table_size\n",
    "        table = np.zeros(table_size, dtype=np.uint32)\n",
    "\n",
    "        print ('Filling unigram table')\n",
    "        p = 0 # Cumulative probability\n",
    "        i = 0\n",
    "        old_i = 0 \n",
    "        for j, unigram in enumerate(vocab):\n",
    "            #print \"j\",j\n",
    "            #print \"unigram\",unigram\n",
    "            \n",
    "            p += float(math.pow(unigram.count, power))/norm\n",
    "            while i < table_size and float(i) / table_size < p:\n",
    "                table[i] = j\n",
    "                i += 1\n",
    "            old_i = i - old_i\n",
    "            sys.stdout.write(\"\\r propability for word '%s' is %f, kept it  %d times\" %(unigram.word,p,old_i))\n",
    "            sys.stdout.flush()\n",
    "            #print(\"propability for word %s is %f, kept it  %d times\" %(unigram.word,p,old_i))\n",
    "        self.table = table\n",
    "    def sample(self, count):\n",
    "        indices = np.random.randint(low=0, high=len(self.table), size=count)\n",
    "        return [self.table[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z): #sigmoid function goes from -6 to +6\n",
    "    if z > 6:\n",
    "        return 1.0\n",
    "    elif z < -6:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1 / (1 + math.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_net(dim, vocab_size):\n",
    "    # Init input words with random numbers from a uniform distribution on the interval [-0.5, 0.5]/dim\n",
    "    tmp = np.random.uniform(low=-0.5/dim, high=0.5/dim, size=(vocab_size, dim))\n",
    "    input_word = tmp \n",
    "    # Init weights with zeros\n",
    "    tmp = np.zeros(shape=(vocab_size, dim))\n",
    "    weights = tmp\n",
    "  \n",
    "\n",
    "    return (input_word, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading word 17000000Total words in training file: 17005209\n",
      "Vocab size: 47135\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary(\"text8\",10) #create vocab with count of skip words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2159669.0869037905\n",
      "Filling unigram table\n",
      " propability for word 'kirchenmusik' is 1.000000, kept it  50420761 times"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:13: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "table = UnigramTable(vocab) #create unigram table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03999149 -0.0206053  -0.05930938  0.05219079  0.05751725]\n",
      " [ 0.01340777  0.03077842  0.06888936  0.00525066 -0.01542601]\n",
      " [-0.00951821  0.03188391 -0.04886503  0.00123507 -0.06706377]\n",
      " ..., \n",
      " [ 0.08776641  0.01561342 -0.03158444  0.02002964 -0.06854255]\n",
      " [-0.02520289  0.02936194  0.01213729  0.0095593  -0.08844167]\n",
      " [ 0.04546868 -0.00848554 -0.08431667  0.00741179  0.00063676]]\n",
      "[[ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "input_word,weights = init_net(5, len(vocab))\n",
    "print (input_word)\n",
    "print ( weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.004859  words Progress: 13700000 of 17005209 (80.56%)"
     ]
    }
   ],
   "source": [
    "\n",
    " #initalize input words and weight matrix with 5 diminension for each word \n",
    "\n",
    "current_word_count=0  #count of words to update alpha\n",
    "alpha_count = 0\n",
    "last_alpha_count = 0\n",
    "starting_alpha=0.025 #initial alpha\n",
    "win=5 #max window size\n",
    "#iter_c = 0 #iteration count\n",
    "with open(\"text8\", 'r') as fi:\n",
    "    for lines in fi:\n",
    "        line = lines.strip()#for each line get the indicies of the words\n",
    "        if not line:\n",
    "            continue\n",
    "        #print (line)\n",
    "        sent = vocab.indices(['<bol>'] + line.split() + ['<eol>'])\n",
    "        #print (\"sent\",sent)\n",
    "        for sent_pos, token in enumerate(sent): #use the position of the \"input word\" to create context window randomly\n",
    "            if current_word_count % 10000 == 0: #update alpha for every 1000 words and print status\n",
    "                alpha_count += (current_word_count - last_alpha_count)\n",
    "                last_alpha_count = current_word_count\n",
    "                alpha = starting_alpha * (1 - float(alpha_count) / vocab.word_count)\n",
    "                if alpha < starting_alpha * 0.0001: alpha = starting_alpha * 0.0001\n",
    "                sys.stdout.write(\"\\rAlpha: %f  words Progress: %d of %d (%.2f%%)\" %\n",
    "                                 (alpha, alpha_count, vocab.word_count,\n",
    "                                  float(alpha_count) / vocab.word_count * 100))\n",
    "                sys.stdout.flush()\n",
    "            current_win = np.random.randint(low=1, high=win+1) #compute the random context\n",
    "            context_start = max(sent_pos - current_win, 0)\n",
    "            context_end = min(sent_pos + current_win + 1, len(sent))\n",
    "            #get the words which are either side of the input word\n",
    "            context = sent[context_start:sent_pos] + sent[sent_pos+1:context_end]\n",
    "            #for every context word\n",
    "            for context_word in context:\n",
    "                error_back = np.zeros(5)\n",
    "                # classifiy inputword with 1 and sample words taken from unigram table as 0\n",
    "                classifiers = [(token, 1)] + [(target, 0) for target in table.sample(5)]\n",
    "                #print(\"classifiers\",classifiers)\n",
    "                \n",
    "                #for every word in classifier\n",
    "                for target, label in classifiers:\n",
    "                    #print (\"\\ttarget\",target)\n",
    "                    #print (\"\\t input word[context_word]\",input_word[context_word])\n",
    "                    #print (\"\\t output word [target]\",weights[target])\n",
    "                    \n",
    "                    #get dot product of input word vector of context word and weight vector of classifier word\n",
    "                    z = np.dot(input_word[context_word], weights[target])\n",
    "                    if math.isnan(z):\n",
    "                        print(\"z is nan\")\n",
    "                        break\n",
    "                    #compute sigmoid of dot product\n",
    "                    p = sigmoid(z)\n",
    "                    if math.isnan(p):\n",
    "                        print (\"p is nan\")\n",
    "                        break\n",
    "                    if math.isnan(alpha):\n",
    "                        print(\"alpha is nan\")\n",
    "                        \n",
    "                    g = alpha * (label - p)\n",
    "                    error_back += g * weights[target]\n",
    "                    #sum up the error computed for every word in the classifier\n",
    "                    weights[target] += g * input_word[context_word]# Update output word\n",
    "                    #print (\"\\tupdated array of target\",target,\"is\",output_word[target])\n",
    "                    #iter_c=iter_c+1\n",
    "                        # Update input_word\n",
    "                #update input word vector of the context using summed error of the classfier\n",
    "                input_word[context_word] += error_back\n",
    "                #print(\"updated array of context word\",context_word,\"is \",input_word[context_word])\n",
    "            current_word_count += 1\n",
    "            #sys.stdout.write(\"\\rcurrent word count %d remaining %d\" %(current_word_count,vocab.word_count-current_word_count))\n",
    "#print (\"iteration count\",iter_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ nan,  nan,  nan,  nan,  nan],\n",
       "       [ nan,  nan,  nan,  nan,  nan],\n",
       "       [ nan,  nan,  nan,  nan,  nan],\n",
       "       ..., \n",
       "       [ nan,  nan,  nan,  nan,  nan],\n",
       "       [ nan,  nan,  nan,  nan,  nan],\n",
       "       [ nan,  nan,  nan,  nan,  nan]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
